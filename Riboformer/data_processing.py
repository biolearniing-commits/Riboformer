
"""
this script converts the wig file (ribosome profiling) into training datasets
"""
import numpy as np
import os
import json
import argparse
from itertools import groupby

# load fasta files
def fasta_iter(fasta_name):
    """
    given a fasta file, yield tuples of header, sequence
    """
    with open(fasta_name) as fasta_file:
        # ditch the boolean (x[0]) and just keep the header or sequence since
        # we know they alternate.
        faiter = (x[1] for x in groupby(fasta_file, lambda line: line[0] == ">"))
        for header in faiter:
            header = header.__next__()[1:].strip() # drop the ">"
            seq = "".join(s.strip() for s in faiter.__next__()) # join all sequences
            print("Sequence loaded.", header)
            yield header, seq


# read wig files
def read_wig(filename, Ribo = 40000):
    '''
    read wig file from filename
    read forward and reverse direction seperately
    Ribo: total number of ribosomes for data normalization (Amin & Zhang et al.,2021)
    '''
    
    # read forward direction
    with open(filename + '_f.wig', 'r') as read_file:
            lines = read_file.readlines()
    Dwig1 = []
    for line in lines[2:]:
        Dwig1.append(line.replace('\n','').replace('\r','').split('\t'))
    Dwig1 = np.array(Dwig1).astype(float)
    
    # read reverse direction
    with open(filename + "_r.wig",'r') as read_file:
            lines2 = read_file.readlines()
    Dwig2 = []
    for line in lines2[2:]:
        Dwig2.append(line.replace('\n','').replace('\r','').split('\t'))
    Dwig2 = np.array(Dwig2).astype(float)

    # normalize the total reads
    normalize = np.sum(Dwig2[:,0]) + np.sum(Dwig1[:,0])
    Dwig = np.zeros([len(Dwig1), 3])
    for i, (d1, d2) in enumerate(zip(Dwig1, Dwig2)):
        Dwig[i, 0] = i
        Dwig[i, 1] = d1[0] * Ribo / normalize
        Dwig[i, 2] = d2[0] * Ribo / normalize
    
    return Dwig


#get the codon density
def sum_adjac(RD):
    return RD[:3 * (len(RD)//3)].reshape(-1,3).sum(1) 


# Generate training dataset
def generate_training(my_data2, seq, Dwig1, Dwig2, wsize, Ctable, P_site = 14):
    '''
    input
    my_data2: gff file of the genome, stores the positions of open reading frames
    seq: DNA sequence of the genome
    Dwig1: sequencing reads from condition 1, generated by read_wig function
    Dwig2: sequencing reads from condition 2, generated by read_wig function
    P_site: 14 for the Mohammad et al., eLife paper
    wsize: length of the context, default 40

    output:
    x_c: feature variable, 0:40 is sequencing reads in the 40 bp window in condition1,
        -40:0 is codon number in the window
    y_c: sequencing read in the target position in condition 2
    z_c: target position in the genome

    comments:
    x_y and y_c are log transformed
    '''
    x_c = []
    y_c = []
    z_c = []
    complement = {'A': 'T', 'C': 'G', 'G': 'C', 'T': 'A'}

    for i, data in enumerate(my_data2):
        
        # number of nt to add in the beginning and end
        n = 30
        gene_len = 200
        RD_thres = 0.003
        
        # 3' is aligned
        if data[2] == 1:
            # positive strand
            if data[1] - data[0] > gene_len:
                RD1 = Dwig1[(int(data[0]) - 1 - n) + P_site : int(data[1]) + n + P_site, 1]
                RD2 = Dwig2[(int(data[0]) - 1 - n) + P_site : int(data[1]) + n + P_site, 1]
                seq_t = seq[int(data[0]) - 1 - n : int(data[1]) + n]
        else:
            # negtive strand
            if data[1] - data[0] > gene_len:
                RD1 = Dwig1[(int(data[0]) - 1 - n) - P_site : int(data[1]) + n - P_site, 2]
                RD1 = RD1[::-1]
                RD2 = Dwig2[(int(data[0]) - 1 - n) - P_site : int(data[1]) + n - P_site, 2]
                RD2 = RD2[::-1]
                seq_t = seq[int(data[0]) - 1 - n : int(data[1]) + n]
                seq_t = ''.join(complement.get(base, base) for base in reversed(seq_t))
        
        
        if data[1] - data[0] > gene_len and np.mean(RD1) > RD_thres:
            codons = list(seq_t[n : n + 3] for n in range(0, len(seq_t), 3))
            codons_v = np.zeros((len(codons),))
            for m, codon in enumerate(codons):
                if codon in Ctable:
                    codons_v[m] = Ctable.index(codon)

            RD1 = sum_adjac(RD1)
            RD2 = sum_adjac(RD2)
            
            rc1 = 10000
            rc2 = 32
            rc3 = 100

            for m in range(int(wsize/2), len(RD1) - int(wsize/2)):
                # training variables
                x_c.append(
                    np.concatenate([np.log2(RD1[m - int(wsize/2) : m + int(wsize/2)] * rc1 + rc2) * rc3,
                                    np.array([m, np.log2(np.mean(RD1[int(wsize/2) : m + int(wsize/2)]) * rc1 + rc2) * rc3]),
                                    codons_v[m - int(wsize/2) : m + int(wsize/2)]])
                )
                
                # prediction variables
                y_c.append(np.log2(RD2[m] * rc1 + rc2) * rc3)
                
                # positions of the calculating codons
                z_c.append([i, m])

    return x_c, y_c, z_c


def main():
    
    parser = argparse.ArgumentParser(
        description = __doc__,
        formatter_class = argparse.RawDescriptionHelpFormatter
    )
    parser.add_argument('-w', '--wsize', default = 40, type = int, 
                        help = 'window size for model training')
    parser.add_argument('-d', '--data_dir', default = '/datasets/GSE119104_Mg_buffer/', type = str,
                        help = 'data folder name')
    parser.add_argument('-r', '--reference', default = 'GSM3358138_filter_Cm_ctrl', type = str,
                        help = 'reference dataset name')
    parser.add_argument('-t', '--target', default = 'GSM3358140_freeze_Mg_ctrl', type = str,
                        help = 'target dataset name')
                
    args = parser.parse_args()

    filepath = {'exp1': args.reference,
                'exp2': args.target,
                'gff_name': 'gene_positions.csv',
                'seq_name': 'genome_sequence.fasta',
                'x_input': 'xc.txt',
                'y_output': 'yc.txt',
                'z_index': 'zc.txt',
                'y_pred': 'ypred.txt',
                }
    parpath = os.path.dirname(os.getcwd())
    for key in filepath.keys():
        filepath[key] = parpath + args.data_dir + filepath[key]

    # load codon table
    with open('codon_table.json') as f:
        Ctable = json.load(f)
        
    complement = {'A': 'T', 'C': 'G', 'G': 'C', 'T': 'A'}
    
    # load gene positions
    print("---------------------------------------------")
    gene_position = np.genfromtxt(filepath['gff_name'], delimiter=",")
    print(f"Gene position loaded. Number of ORF is {len(gene_position)}")

    # load genome sequences
    print("---------------------------------------------")
    fiter = fasta_iter(filepath['seq_name'])
    for _, seq in fiter:
        print(f"Sequence length: {len(seq)}")

    # load ribosome footprints
    print("---------------------------------------------")
    Dwig1 = read_wig(filepath['exp1'])
    Dwig2 = read_wig(filepath['exp2'])
    print("Finish reading the ribosome footprints.")

    # generate training datasets
    print("---------------------------------------------")
    x_c, y_c, z_c = generate_training(gene_position, seq, Dwig1, Dwig2, args.wsize, Ctable, 14)
    print("Finish generating the training datasets.")

    np.savetxt(filepath['x_input'], x_c, delimiter="\t")
    np.savetxt(filepath['y_output'], y_c, delimiter="\t")
    np.savetxt(filepath['z_index'], z_c, delimiter="\t")
    
    print(f"Datasets saved. size: {len(x_c)}.")

    

if __name__ == '__main__':
    main()
