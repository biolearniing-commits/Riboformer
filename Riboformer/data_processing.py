
"""
this script converts the wig file (ribosome profiling) into training datasets
"""
import numpy as np
import os
import json
import argparse
from itertools import groupby
from Bio import SeqIO
from BCBio import GFF

# load fasta files
def fasta_iter(fasta_name):
    """
    given a fasta file, yield tuples of header, sequence
    """
    with open(fasta_name) as fasta_file:
        # ditch the boolean (x[0]) and just keep the header or sequence since
        # we know they alternate.
        faiter = (x[1] for x in groupby(fasta_file, lambda line: line[0] == ">"))
        for header in faiter:
            header = header.__next__()[1:].strip() # drop the ">"
            seq = "".join(s.strip() for s in faiter.__next__()) # join all sequences
            print("Sequence loaded.", header)
            yield header, seq


# read wig files
def read_wig(filename):
    '''
    read wig file from filename
    read forward and reverse direction seperately
    Ribo: total number of ribosomes for data normalization (Amin & Zhang et al.,2021)
    '''
    
    # read forward direction
    wig1 = {}
    chrom_all = []
    with open(filename + '_f.wig', 'r') as read_file:
        lines = read_file.readlines()
        for j in range(1, len(lines)):
            if lines[j][0] == 'f':
                chrom_all.append(lines[j].split(' ')[2].split('=')[1].replace('chr', ''))
                if j > 2:
                    wig1[chrom_all[-2]] = np.array(read)
                read = []
            else:
                read.append(float(lines[j]))
    wig1[chrom_all[-1]] = np.array(read)
    
    wig2 = {}
    chrom_all = []
    with open(filename + '_r.wig', 'r') as read_file:
        lines = read_file.readlines()
        for j in range(1, len(lines)):
            if lines[j][0] == 'f':
                chrom_all.append(lines[j].split(' ')[2].split('=')[1].replace('chr', ''))
                if j > 2:
                    wig2[chrom_all[-2]] = np.array(read)
                read = []
            else:
                read.append(float(lines[j]))
    wig2[chrom_all[-1]] = np.array(read)
        
    wig_total = {}
    for c in wig1.keys():
        wig_total[c] = np.vstack([np.zeros(wig1[c].shape), \
                             wig1[c], \
                             wig2[c]] ).transpose()
    return wig_total


#get the codon density
def sum_adjac(RD):
    return RD[:3 * (len(RD)//3)].reshape(-1,3).sum(1) 


# Generate training dataset
def generate_training(my_data2, seq, Dwig1, Dwig2, wsize, Ctable, P_site = 14, thres = 25):
    '''
    input
    my_data2: gff file of the genome, stores the positions of open reading frames
    seq: DNA sequence of the genome
    Dwig1: sequencing reads from condition 1, generated by read_wig function
    Dwig2: sequencing reads from condition 2, generated by read_wig function
    P_site: 14 for the Mohammad et al., eLife paper
    wsize: length of the context, default 40
    thres: use the top % of highly expressed genes

    output:
    x_c: feature variable, 0:40 is sequencing reads in the 40 bp window in condition1,
        -40:0 is codon number in the window
    y_c: sequencing read in the target position in condition 2
    z_c: target position in the genome

    comments:
    x_y and y_c are log transformed
    '''
    x_c = []
    y_c = []
    z_c = []
    complement = {'A': 'T', 'C': 'G', 'G': 'C', 'T': 'A'}
    
    RD_mean_list = []
    for i, data in enumerate(my_data2):
        
        # number of nt to add in the beginning and end
        n = 30
        gene_len = 200
        
        # 3' is aligned
        if data[2] == 1:
            # positive strand
            if data[1] - data[0] > gene_len:
                RD1 = Dwig1[(int(data[0]) - 1 - n) + P_site : int(data[1]) + n + P_site, 1]
        else:
            # negtive strand
            if data[1] - data[0] > gene_len:
                RD1 = Dwig1[(int(data[0]) - 1 - n) - P_site : int(data[1]) + n - P_site, 2]
                RD1 = RD1[::-1]
        if data[1] - data[0] > gene_len:
            RD_mean_list.append(np.mean(RD1))
            
    RD_thres1 = np.percentile(RD_mean_list, 100 - thres)
    normal = 0.005/np.mean(RD_mean_list)

    for i, data in enumerate(my_data2):
        
        # number of nt to add in the beginning and end
        n = 30
        gene_len = 200
        
        # 3' is aligned
        if data[2] == 1:
            # positive strand
            if data[1] - data[0] > gene_len:
                RD1 = Dwig1[(int(data[0]) - 1 - n) + P_site : int(data[1]) + n + P_site, 1]
                RD2 = Dwig2[(int(data[0]) - 1 - n) + P_site : int(data[1]) + n + P_site, 1]
                seq_t = seq[int(data[0]) - 1 - n : int(data[1]) + n]
        else:
            # negtive strand
            if data[1] - data[0] > gene_len:
                RD1 = Dwig1[(int(data[0]) - 1 - n) - P_site : int(data[1]) + n - P_site, 2]
                RD1 = RD1[::-1]
                RD2 = Dwig2[(int(data[0]) - 1 - n) - P_site : int(data[1]) + n - P_site, 2]
                RD2 = RD2[::-1]
                seq_t = seq[int(data[0]) - 1 - n : int(data[1]) + n]
                seq_t = ''.join(complement.get(base, base) for base in reversed(seq_t))
        
        
        if data[1] - data[0] > gene_len and np.mean(RD1) > RD_thres1:
            codons = list(seq_t[n : n + 3] for n in range(0, len(seq_t), 3))
            codons_v = np.zeros((len(codons),))
            for m, codon in enumerate(codons):
                if codon in Ctable:
                    codons_v[m] = Ctable.index(codon)

            RD1 = sum_adjac(RD1)
            RD2 = sum_adjac(RD2)
            
            rc1 = 10000*normal
            rc2 = 32
            rc3 = 100

            for m in range(int(wsize/2), len(RD1) - int(wsize/2)):
                # training variables
                x_c.append(
                    np.concatenate([np.log2(RD1[m - int(wsize/2) : m + int(wsize/2)] * rc1 + rc2) * rc3,
                                    np.array([m, np.log2(np.mean(RD1[int(wsize/2) : m + int(wsize/2)]) * rc1 + rc2) * rc3]),
                                    codons_v[m - int(wsize/2) : m + int(wsize/2)]])
                )
                
                # prediction variables
                y_c.append(np.log2(RD2[m] * rc1 + rc2) * rc3)
                
                # positions of the calculating codons
                z_c.append([i, m])

    return x_c, y_c, z_c


def main():
    
    parser = argparse.ArgumentParser(
        description = __doc__,
        formatter_class = argparse.RawDescriptionHelpFormatter
    )
    parser.add_argument('-w', '--wsize', default = 40, type = int, 
                        help = 'window size (AA) for model training')
    parser.add_argument('-th', '--threshold', default = 25, type = int, 
                        help = 'expression threshold (percentile) for model training')
    parser.add_argument('-d', '--data_dir', default = 'GSE119104_Mg_buffer', type = str,
                        help = 'data folder name')
    parser.add_argument('-r', '--reference', default = 'GSM3358138_filter_Cm_ctrl', type = str,
                        help = 'reference dataset name')
    parser.add_argument('-p', '--psite', default = 14, type = int,
                        help = 'offset for p site position')
    parser.add_argument('-t', '--target', default = 'GSM3358140_freeze_Mg_ctrl', type = str,
                        help = 'target dataset name')
                
    args = parser.parse_args()

    filepath = {'exp1': args.reference,
                'exp2': args.target,
                'x_input': 'xc.txt',
                'y_output': 'yc.txt',
                'z_index': 'zc.txt',
                'y_pred': 'ypred.txt',
                }
    parpath = os.path.dirname(os.getcwd())
    datapath = parpath + '/datasets/' + args.data_dir + '/'
    for key in filepath.keys():
        filepath[key] = datapath + filepath[key]
    
    all_files = os.listdir(datapath)
    fasta_file = [f for f in all_files if f.endswith('.fasta')]
    gff_file = [f for f in all_files if f.endswith('.gff3')]

    # load codon table
    with open('codon_table.json') as f:
        Ctable = json.load(f)
        
    complement = {'A': 'T', 'C': 'G', 'G': 'C', 'T': 'A'}

    # load ribosome footprints
    print("---------------------------------------------")
    print("Loading ribosome densities.")
    Dwig1 = read_wig(filepath['exp1'])
    Dwig2 = read_wig(filepath['exp2'])
    print("Finish reading the ribosome footprints.")

    # load genome sequences
    print("---------------------------------------------")
    print("Loading genome sequences.")
    seq_dict = {}
    for record in SeqIO.parse(datapath + fasta_file[0], "fasta"):
        seq_dict[record.id] = record.seq
        print(record.id + " len is " + str(len(record.seq)))

    # generate training datasets
    print("---------------------------------------------")
    x_c_total = []
    y_c_total = []
    z_c_total = []
    z_gene = 0
    for key in seq_dict.keys():
        if key != 'Mito':
            limit_info = dict(gff_id=[key], gff_type=["CDS"])
            in_handle = open(datapath + gff_file[0])
            gff_data = []
            for rec in GFF.parse(in_handle, limit_info=limit_info):
                for j in range(len(rec.features)):
                    # position need to plus 1 before passing to the generate training set function
                    # seq[start.position: end.position] is the right index for the DNA sequence
                    gff_data.append([rec.features[j].location.start.position + 1,
                                     rec.features[j].location.end.position,
                                     rec.features[j].location.strand])
            in_handle.close()
            gff_data = np.array(gff_data)

            x_c, y_c, z_c = generate_training(gff_data, seq_dict[key], Dwig1[key], Dwig2[key], args.wsize, Ctable, int(args.psite),
                                              float(args.threshold))
            x_c_total = x_c_total + x_c
            y_c_total = y_c_total + y_c
            z_c_total = z_c_total + z_c
            z_gene += len(np.unique(np.array(z_c)[:, 0]))

    x_c_total = np.array(x_c_total)
    y_c_total = np.array(y_c_total)
    z_c_total = np.array(z_c_total)
    print(f"Finish generating the training datasets. Number of unique genes is {z_gene}")

    np.savetxt(filepath['x_input'], x_c_total, delimiter="\t")
    np.savetxt(filepath['y_output'], y_c_total, delimiter="\t")
    np.savetxt(filepath['z_index'], z_c_total, delimiter="\t")
    
    print(f"Datasets saved. Number of samples is {len(x_c)}.")

    

if __name__ == '__main__':
    main()
